# -*- coding: utf-8 -*-
"""MaskedTransformer+RagScratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17tOopHJJWI4-X19T-G06-TYaOwNG5SCs
"""

import os
import re

def preprocess_text(text):
  # Replace multiple spaces with single spaces.
  text = re.sub(r'\s+', ' ', text)

  # Keep only letters, numbers, and spaces.
  text = re.sub(r'[^a-zA-Z0-9 ]', '', text)

  return text

def chunk_text(text, chunk_size=512, overlap=30):
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk = words[i:i + chunk_size]
        chunks.append(' '.join(chunk))
        i += chunk_size - overlap  # step forward
    return chunks

# Load your legal document (txt file)
with open("/content/sample dataset.txt", "r", encoding="utf-8") as file:
    full_text = file.read()



# Chunk it
chunks = chunk_text(preprocess_text(full_text))

# Save the chunks to files (or keep in memory)
for idx, chunk in enumerate(chunks):
    with open(f"chunk_{idx}.txt", "w", encoding="utf-8") as f:
        f.write(chunk)

print(f"Created {len(chunks)} chunks.")

!pip install --upgrade sympy



from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch



# Load pretrained T5 model
tokenizer = T5Tokenizer.from_pretrained("valhalla/t5-small-qg-hl")
model = T5ForConditionalGeneration.from_pretrained("valhalla/t5-small-qg-hl")

def generate_question(context):
    input_text = f"generate question: {context}"
    inputs = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    outputs = model.generate(inputs, max_length=64, num_beams=4, early_stopping=True)
    question = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return question

# Example: generate one question per chunk
for i, chunk in enumerate(chunks[:2]):  # demo for first 5 chunks
    question = generate_question(chunk)
    print(f"\nChunk {i+1}:\nQ: {question}\nContext: {chunk[:]}...")

from transformers import pipeline
import torch

device = 0 if torch.cuda.is_available() else -1

# Load a question-answering pipeline
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Example: generate QA pair from chunk
qa_pairs = []

for i, chunk in enumerate(chunks):
    question = generate_question(chunk)
    result = qa_pipeline(question=question, context=chunk)
    answer = result['answer']

    qa_pairs.append({
        "question": question,
        "answer": answer,
        "context": chunk
    })

# Preview one
print(qa_pairs[0])

import pandas as pd

df = pd.DataFrame(qa_pairs)
df.to_csv("qa_pairs.csv", index=False)
df.head(5)
# print("question")
# print(df['question'][1])
# print("answer")
# print(df['answer'][1])
# print("context")
# print(df['context'][1])

from tokenizers import Tokenizer, models, trainers, pre_tokenizers

# Initialize BPE tokenizer
tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

# Define special tokens
trainer = trainers.BpeTrainer(special_tokens=["<s>", "</s>", "<pad>", "<unk>", "<mask>"])

# Give the path to your single file
file_path = "/content/sample dataset.txt"  # Update with your actual file name

# Train tokenizer
tokenizer.train(files=[file_path], trainer=trainer)

# Save the tokenizer
tokenizer.save("legal_tokenizer.json")

tokenizer = Tokenizer.from_file("legal_tokenizer.json")


print("Vocab size:", tokenizer.get_vocab_size())

sample_text = "one cast law"
encoding = tokenizer.encode(sample_text)

print("Tokens:", encoding.tokens)
print("IDs:", encoding.ids)

# Hyperparameters
VOCAB_SIZE = tokenizer.get_vocab_size()   # Update based on your tokenizer's vocab (2220 in your case)
MAX_LEN = 512       # Max input length
EMBED_DIM = 256     # Embedding size
NUM_HEADS = 8       # Number of attention heads
FF_DIM = 512        # Feedforward network size
NUM_LAYERS = 4      # Number of encoder and decoder layers
DROPOUT = 0.1

print(VOCAB_SIZE)

import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, embed_dim, max_len=512): # You might want to increase max_len here
        super().__init__()
        pe = torch.zeros(max_len, embed_dim)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # Shape: (1, max_len, embed_dim)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: (batch_size, seq_len, embed_dim)
        # If sequence length exceeds max_len, truncate positional encodings
        seq_len = x.size(1)
        #Adjusting the max_len parameter or truncating/padding sequences to ensure compatibility.
        #Increasing max_len to accommodate the longer sequence.
        pe = self.pe[:, :min(seq_len, self.pe.size(1))] # Truncate if needed
        if seq_len > self.pe.size(1):
            # If the sequence length is greater than max_len, repeat the positional encodings
            # to cover the entire sequence length
            num_repeats = seq_len // self.pe.size(1) + 1
            pe = self.pe.repeat(1, num_repeats, 1)[:, :seq_len, :]

        return x + pe # Add the positional encodings

import torch.nn.functional as F

class EncoderBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)

        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, embed_dim)
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, src_mask=None):
        # Self-attention
        attn_output, _ = self.self_attn(x, x, x, key_padding_mask=src_mask)
        x = self.norm1(x + self.dropout(attn_output))  # Residual + Norm

        # Feedforward
        ff_output = self.ff(x)
        x = self.norm2(x + self.dropout(ff_output))  # Residual + Norm
        return x

class DecoderBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)

        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.norm3 = nn.LayerNorm(embed_dim)

        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, embed_dim)
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_output, tgt_mask=None, memory_mask=None):
        # 1. Masked Self-Attention (for decoder input)
        attn1, _ = self.self_attn(x, x, x, attn_mask=tgt_mask)
        x = self.norm1(x + self.dropout(attn1))

        # 2. Cross-Attention (decoder attends to encoder output)
        attn2, _ = self.cross_attn(x, enc_output, enc_output, attn_mask=memory_mask)
        x = self.norm2(x + self.dropout(attn2))

        # 3. Feedforward
        ff_out = self.ff(x)
        x = self.norm3(x + self.dropout(ff_out))
        return x

class TransformerModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=512, num_heads=8, ff_dim=2048, num_layers=6, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim

        # Embeddings
        self.src_embedding = nn.Embedding(vocab_size, embed_dim)
        self.tgt_embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_encoder = PositionalEncoding(embed_dim)

        # Encoder
        self.encoder_layers = nn.ModuleList([
            EncoderBlock(embed_dim, num_heads, ff_dim, dropout)
            for _ in range(num_layers)
        ])

        # Decoder
        self.decoder_layers = nn.ModuleList([
            DecoderBlock(embed_dim, num_heads, ff_dim, dropout)
            for _ in range(num_layers)
        ])

        # Final output layer
        self.output_layer = nn.Linear(embed_dim, vocab_size)

    def forward(self, input_ids, decoder_input_ids, src_mask=None, tgt_mask=None, memory_mask=None): # Modified to take input_ids and decoder_input_ids
        # Embed and add position
        # Assume input_ids are the source sequence
        src = self.pos_encoder(self.src_embedding(input_ids))
        # Assume decoder_input_ids are the target sequence
        tgt = self.pos_encoder(self.tgt_embedding(decoder_input_ids))

        # Encoder
        for layer in self.encoder_layers:
            src = layer(src, src_mask)

        # Decoder
        output = tgt
        for layer in self.decoder_layers:
            output = layer(output, src, tgt_mask, memory_mask)

        return self.output_layer(output) # Changed to return output logits directly

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader

class LegalQADatasetCSV(Dataset):
    def __init__(self, path, tokenizer, max_len=512):
        self.data = pd.read_csv(path)
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        question = self.data.loc[idx, 'question']
        answer = self.data.loc[idx, 'answer']
        context = self.data.loc[idx, 'context']

        src_text = question + " [SEP] " + context
        tgt_text = answer

        src_tokens = self.tokenizer.encode(src_text).ids[:self.max_len]
        tgt_tokens = self.tokenizer.encode(tgt_text).ids[:self.max_len]

        decoder_input = [self.tokenizer.token_to_id("<s>")] + tgt_tokens[:-1]
        decoder_target = tgt_tokens

        return {
            "src": torch.tensor(src_tokens, dtype=torch.long),
            "tgt_in": torch.tensor(decoder_input, dtype=torch.long),
            "tgt_out": torch.tensor(decoder_target, dtype=torch.long)
        }

from torch.nn.utils.rnn import pad_sequence # Import pad_sequence

def collate_fn(batch):
    src_batch = [item["src"] for item in batch]
    tgt_in_batch = [item["tgt_in"] for item in batch]
    tgt_out_batch = [item["tgt_out"] for item in batch]

    src_pad = pad_sequence(src_batch, batch_first=True, padding_value=tokenizer.token_to_id("<pad>"))
    tgt_in_pad = pad_sequence(tgt_in_batch, batch_first=True, padding_value=tokenizer.token_to_id("<pad>"))
    tgt_out_pad = pad_sequence(tgt_out_batch, batch_first=True, padding_value=tokenizer.token_to_id("<pad>"))

    # Return the padded tensors, not the keys
    return src_pad, tgt_in_pad, tgt_out_pad

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import AdamW

# ⚙️ Hyperparameters
batch_size = 8
epochs = 10
learning_rate = 3e-4
#Instead of -1, set to 'cpu' if cuda is not available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
pad_id = tokenizer.token_to_id("<pad>")


# 🧱 Create dataset and dataloader
dataset = LegalQADatasetCSV("/content/qa_pairs.csv", tokenizer)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)



# 🧠 Model, loss, optimizer
model = TransformerModel(vocab_size=tokenizer.get_vocab_size(), embed_dim=512, num_heads=8, num_layers=6).to(device)

# Changed 'd_model' to 'embed_dim'

# criterion = nn.CrossEntropyLoss(ignore_index=pad_id)
criterion= nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id("<pad>"),label_smoothing=0.1)

optimizer = AdamW(model.parameters(), lr=learning_rate)

import torch

def generate_square_subsequent_mask(sz):
    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask

# Inside your training loop:
for epoch in range(10):
    # ... (other code)

    for batch in dataloader:
        # unpack the batch here (src, tgt_in, tgt_out)
        src, tgt_in, tgt_out = batch
        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)


        # 📌 Generate attention masks (Corrected)
        src_mask = (src == pad_id)  # shape: (B, S)
        tgt_mask = (tgt_in == pad_id)  # shape: (B, T)

        # Generate square causal mask for decoder self-attention
        tgt_seq_len = tgt_in.shape[1]
        tgt_attn_mask = generate_square_subsequent_mask(tgt_seq_len).to(device)

        # 🔄 Forward pass
        output = model(src, tgt_in, src_mask=src_mask, tgt_mask=tgt_attn_mask)  # Pass tgt_attn_mask here
        # ... (rest of the training loop)

for epoch in range(50):
    model.train()
    total_loss = 0

    for batch in dataloader:
        src, tgt_in, tgt_out = batch
        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)

        # 📌 Generate attention masks
        src_mask = (src == pad_id)  # shape: (B, S)
        tgt_mask = generate_square_subsequent_mask(tgt_in.shape[1]).type(torch.bool).to(device) # shape: (T, T)

        # 🔄 Forward pass
        output = model(src, tgt_in, src_mask=src_mask, tgt_mask=tgt_mask)

        # 🎯 Flatten for loss computation
        output = output.view(-1, output.size(-1))
        tgt_out = tgt_out.view(-1)

        loss = criterion(output, tgt_out)

        # 🔁 Backward + optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(dataloader)

    print(f"Epoch [{epoch+1}/{100}] - Loss: {avg_loss:.4f}")

torch.save(model.state_dict(), "transformer_legalQA1.pt")

def generate_answer(model, tokenizer, question, context, max_len=64):
    model.eval()
    with torch.no_grad():
        src_text = question + " [SEP] " + context
        src_ids = torch.tensor([tokenizer.encode(src_text).ids]).to(device)

        generated = [tokenizer.token_to_id("<s>")]

        for _ in range(max_len):
            tgt_tensor = torch.tensor([generated]).to(device)
            output = model(input_ids=src_ids, decoder_input_ids=tgt_tensor)
            next_token_logits = output[:, -1, :]

            # Greedy decoding: Select the token with the highest probability
            next_token = torch.argmax(next_token_logits, dim=-1).item()

            generated.append(next_token)

            if next_token == tokenizer.token_to_id("</s>"):
                break

        decoded = tokenizer.decode(generated[1:])  # skip <s>
        return decoded

!pip install -q faiss-cpu sentence-transformers
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer


# 1. Create a SentenceTransformer for embeddings
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# 2. Embed all your document chunks and create a FAISS index
document_chunks = chunks  # Assuming all_contexts holds your chunked data
document_embeddings = embedder.encode(document_chunks, convert_to_tensor=True)
document_embeddings = document_embeddings.cpu().numpy()  # Convert to NumPy for FAISS

dimension = document_embeddings.shape[1]
faiss_index = faiss.IndexFlatL2(dimension)
faiss_index.add(document_embeddings)



def retrieve_and_generate(query, model, tokenizer, k=5):
    # 🔍 Step 1: Embed the query using a SentenceTransformer
    query_vec = embedder.encode(query)

    # 🧠 Step 2: Search FAISS index
    _, top_k_idx = faiss_index.search(np.array([query_vec]), k=k)

    # 🧾 Step 3: Retrieve top-k chunks
    retrieved_chunks = [document_chunks[i] for i in top_k_idx[0]]

    # 📃 Step 4: Concatenate retrieved chunks
    combined_context = " ".join(retrieved_chunks)

    # 🧠 Step 5: Use query + retrieved context as input to your Transformer
    # Call generate_answer, which uses model.forward()
    return generate_answer(model, tokenizer, query, combined_context, max_len=100)

# ... (rest of your code, including the call to retrieve_and_generate)

k=input()
print(retrieve_and_generate(k, model, tokenizer, k=3))

